{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_dft_features(signal_data, n_fft=2048):\n",
    "    \"\"\"\n",
    "    Extract DFT magnitude spectrum features as described in the research paper.\n",
    "    \n",
    "    Parameters:\n",
    "        signal_data: Input signal data\n",
    "        n_fft: Number of points for FFT (default: 2048 as used in the paper)\n",
    "    \n",
    "    Returns:\n",
    "        Magnitude spectrum features\n",
    "    \"\"\"\n",
    "   \n",
    "    dft = np.fft.fft(signal_data, n=n_fft)\n",
    "    \n",
    "   \n",
    "    magnitude_spectrum = np.abs(dft[:n_fft//2])\n",
    "    \n",
    "    \n",
    "    if np.max(magnitude_spectrum) > 0:\n",
    "        magnitude_spectrum = magnitude_spectrum / np.max(magnitude_spectrum)\n",
    "    \n",
    "    return magnitude_spectrum\n",
    "\n",
    "\n",
    "# def moving_average_filter(data, window_size=5):\n",
    "#     \"\"\"\n",
    "#     Applies a moving average filter to smooth the input data.\n",
    "#     \"\"\"\n",
    "#     return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segment_pair_files(folder_path):\n",
    "    \"\"\"\n",
    "    Groups CSV files in the given folder into segments by pairing the low band (L)\n",
    "    and high band (H) parts.\n",
    "    \"\"\"\n",
    "    segment_dict = {}\n",
    "    pattern = re.compile(r\"(\\d+)([LH])_(\\d+)\\.csv\")\n",
    "    files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    \n",
    "    for file in files:\n",
    "        basename = os.path.basename(file)\n",
    "        match = pattern.match(basename)\n",
    "        if match:\n",
    "            seg_id = match.group(1)\n",
    "            band = match.group(2)   # 'L' or 'H'\n",
    "            pair_idx = match.group(3)\n",
    "            key = (seg_id, pair_idx)\n",
    "            if key not in segment_dict:\n",
    "                segment_dict[key] = {}\n",
    "            segment_dict[key][band] = file\n",
    "    \n",
    "    segments = []\n",
    "    for key, parts in segment_dict.items():\n",
    "        if 'L' in parts and 'H' in parts:\n",
    "            try:\n",
    "                data_low = np.loadtxt(parts['L'], delimiter=',')\n",
    "                data_high = np.loadtxt(parts['H'], delimiter=',')\n",
    "                segments.append((data_low, data_high, parts['L'], parts['H']))\n",
    "                print(f\"Loaded pair: {os.path.basename(parts['L'])} and {os.path.basename(parts['H'])}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading segment {key}: {e}\")\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_all_features(signal_low, signal_high, fs):\n",
    "    \"\"\"\n",
    "    Extract features strictly following the research paper:\n",
    "    use only the lower band DFT magnitude spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "        signal_low: Low band signal data\n",
    "        signal_high: High band signal data (ignored)\n",
    "        fs: Sampling rate (not used in this extraction)\n",
    "    \n",
    "    Returns:\n",
    "        Feature vector of length 1024 (one-sided DFT magnitude spectrum)\n",
    "    \"\"\"\n",
    "    # filtered_low = moving_average_filter(signal_low)\n",
    "    dft_features_low = extract_dft_features(signal_low)\n",
    "    return dft_features_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_csv_data_to_training(csv_path, X, y, class_label=0, fs=40e6, t_seg=20):\n",
    "    \"\"\"\n",
    "    Add data from a CSV file to the training dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        csv_path: Path to the CSV file with magnitude data\n",
    "        X: Feature vectors (list or numpy array)\n",
    "        y: Labels (list or numpy array)\n",
    "        class_label: Label to assign to this data (0 for background/non-drone, 1 for drone)\n",
    "        fs: Sampling rate in Hz\n",
    "        t_seg: Time segment in ms\n",
    "    \n",
    "    Returns:\n",
    "        Updated X and y as numpy arrays\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Convert X and y to lists if they are numpy arrays\n",
    "        X_list = X.tolist() if isinstance(X, np.ndarray) else list(X)\n",
    "        y_list = y.tolist() if isinstance(y, np.ndarray) else list(y)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract magnitude data\n",
    "        magnitude_data = df['Magnitude'].values\n",
    "        \n",
    "        # Calculate segment size\n",
    "        samples_per_band = int(t_seg / 1000 * fs)\n",
    "        segment_length = 2 * samples_per_band\n",
    "        \n",
    "        # Calculate number of complete segments\n",
    "        total_samples = len(magnitude_data)\n",
    "        num_segments = total_samples // segment_length\n",
    "        \n",
    "        print(f\"Found {num_segments} complete segments in CSV file\")\n",
    "        \n",
    "       \n",
    "        for i in range(num_segments):\n",
    "            segment = magnitude_data[i * segment_length : (i + 1) * segment_length]\n",
    "            \n",
    "       \n",
    "            low_band = segment[:samples_per_band]\n",
    "            high_band = segment[samples_per_band:]\n",
    "            \n",
    "         \n",
    "            feature_vector = extract_all_features(low_band, high_band, fs)\n",
    "            \n",
    "            # Add to training data\n",
    "            X_list.append(feature_vector)\n",
    "            y_list.append(class_label)\n",
    "        \n",
    "        print(f\"Added {num_segments} segments with class label {class_label} to training data\")\n",
    "        \n",
    "\n",
    "        return np.array(X_list), np.array(y_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV file: {e}\")\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drone_folders = [\n",
    "    'Data\\\\DroneRF\\\\Bepop drone',\n",
    "    'Data\\\\DroneRF\\\\Phantom drone',\n",
    "    'Data\\\\DroneRF\\\\AR drone'\n",
    "]\n",
    "background_folder = 'Data\\DroneRF\\Background RF activites'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 40e6  # Sampling rate: 40 MHz\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading drone segments from Data\\DroneRF\\Bepop drone\n",
      "Loaded pair: 10000L_0.csv and 10000H_0.csv\n",
      "Loaded pair: 10000L_1.csv and 10000H_1.csv\n",
      "Loaded pair: 10000L_10.csv and 10000H_10.csv\n",
      "Loaded pair: 10000L_11.csv and 10000H_11.csv\n",
      "Loaded pair: 10000L_12.csv and 10000H_12.csv\n",
      "Loaded pair: 10000L_13.csv and 10000H_13.csv\n",
      "Loaded pair: 10000L_14.csv and 10000H_14.csv\n",
      "Loaded pair: 10000L_15.csv and 10000H_15.csv\n",
      "Loaded pair: 10000L_16.csv and 10000H_16.csv\n",
      "Loaded pair: 10000L_17.csv and 10000H_17.csv\n",
      "Loaded pair: 10000L_18.csv and 10000H_18.csv\n",
      "Loaded pair: 10000L_19.csv and 10000H_19.csv\n",
      "Loaded pair: 10000L_2.csv and 10000H_2.csv\n",
      "Loaded pair: 10000L_20.csv and 10000H_20.csv\n",
      "Loaded pair: 10000L_3.csv and 10000H_3.csv\n",
      "Loaded pair: 10000L_4.csv and 10000H_4.csv\n",
      "Loaded pair: 10000L_5.csv and 10000H_5.csv\n",
      "Loaded pair: 10000L_6.csv and 10000H_6.csv\n",
      "Loaded pair: 10000L_7.csv and 10000H_7.csv\n",
      "Loaded pair: 10000L_8.csv and 10000H_8.csv\n",
      "Loaded pair: 10000L_9.csv and 10000H_9.csv\n",
      "Loaded pair: 10001L_0.csv and 10001H_0.csv\n",
      "Loaded pair: 10001L_1.csv and 10001H_1.csv\n",
      "Loaded pair: 10001L_10.csv and 10001H_10.csv\n",
      "Loaded pair: 10001L_11.csv and 10001H_11.csv\n",
      "Loaded pair: 10001L_12.csv and 10001H_12.csv\n",
      "Loaded pair: 10001L_13.csv and 10001H_13.csv\n",
      "Loaded pair: 10001L_14.csv and 10001H_14.csv\n",
      "Loaded pair: 10001L_15.csv and 10001H_15.csv\n",
      "Loaded pair: 10001L_16.csv and 10001H_16.csv\n",
      "Loaded pair: 10001L_17.csv and 10001H_17.csv\n",
      "Loaded pair: 10001L_18.csv and 10001H_18.csv\n",
      "Loaded pair: 10001L_19.csv and 10001H_19.csv\n",
      "Loaded pair: 10001L_2.csv and 10001H_2.csv\n",
      "Loaded pair: 10001L_20.csv and 10001H_20.csv\n",
      "Loaded pair: 10001L_3.csv and 10001H_3.csv\n",
      "Loaded pair: 10001L_4.csv and 10001H_4.csv\n",
      "Loaded pair: 10001L_5.csv and 10001H_5.csv\n",
      "Loaded pair: 10001L_6.csv and 10001H_6.csv\n",
      "Loaded pair: 10001L_7.csv and 10001H_7.csv\n",
      "Loaded pair: 10001L_8.csv and 10001H_8.csv\n",
      "Loaded pair: 10001L_9.csv and 10001H_9.csv\n",
      "Loaded pair: 10010L_0.csv and 10010H_0.csv\n",
      "Loaded pair: 10010L_1.csv and 10010H_1.csv\n",
      "Loaded pair: 10010L_10.csv and 10010H_10.csv\n",
      "Loaded pair: 10010L_11.csv and 10010H_11.csv\n",
      "Loaded pair: 10010L_12.csv and 10010H_12.csv\n",
      "Loaded pair: 10010L_13.csv and 10010H_13.csv\n",
      "Loaded pair: 10010L_14.csv and 10010H_14.csv\n",
      "Loaded pair: 10010L_15.csv and 10010H_15.csv\n",
      "Loaded pair: 10010L_16.csv and 10010H_16.csv\n",
      "Loaded pair: 10010L_17.csv and 10010H_17.csv\n",
      "Loaded pair: 10010L_18.csv and 10010H_18.csv\n",
      "Loaded pair: 10010L_19.csv and 10010H_19.csv\n",
      "Loaded pair: 10010L_2.csv and 10010H_2.csv\n",
      "Loaded pair: 10010L_20.csv and 10010H_20.csv\n",
      "Loaded pair: 10010L_3.csv and 10010H_3.csv\n",
      "Loaded pair: 10010L_4.csv and 10010H_4.csv\n",
      "Loaded pair: 10010L_5.csv and 10010H_5.csv\n",
      "Loaded pair: 10010L_6.csv and 10010H_6.csv\n",
      "Loaded pair: 10010L_7.csv and 10010H_7.csv\n",
      "Loaded pair: 10010L_8.csv and 10010H_8.csv\n",
      "Loaded pair: 10010L_9.csv and 10010H_9.csv\n",
      "Loaded pair: 10011L_0.csv and 10011H_0.csv\n",
      "Loaded pair: 10011L_1.csv and 10011H_1.csv\n",
      "Loaded pair: 10011L_10.csv and 10011H_10.csv\n",
      "Loaded pair: 10011L_11.csv and 10011H_11.csv\n",
      "Loaded pair: 10011L_12.csv and 10011H_12.csv\n",
      "Loaded pair: 10011L_13.csv and 10011H_13.csv\n",
      "Loaded pair: 10011L_14.csv and 10011H_14.csv\n",
      "Loaded pair: 10011L_15.csv and 10011H_15.csv\n",
      "Loaded pair: 10011L_16.csv and 10011H_16.csv\n",
      "Loaded pair: 10011L_17.csv and 10011H_17.csv\n",
      "Loaded pair: 10011L_18.csv and 10011H_18.csv\n",
      "Loaded pair: 10011L_19.csv and 10011H_19.csv\n",
      "Loaded pair: 10011L_2.csv and 10011H_2.csv\n",
      "Loaded pair: 10011L_20.csv and 10011H_20.csv\n",
      "Loaded pair: 10011L_3.csv and 10011H_3.csv\n",
      "Loaded pair: 10011L_4.csv and 10011H_4.csv\n",
      "Loaded pair: 10011L_5.csv and 10011H_5.csv\n",
      "Loaded pair: 10011L_6.csv and 10011H_6.csv\n",
      "Loaded pair: 10011L_7.csv and 10011H_7.csv\n",
      "Loaded pair: 10011L_8.csv and 10011H_8.csv\n",
      "Loaded pair: 10011L_9.csv and 10011H_9.csv\n",
      "Loading drone segments from Data\\DroneRF\\Phantom drone\n",
      "Loaded pair: 11000L_0.csv and 11000H_0.csv\n",
      "Loaded pair: 11000L_1.csv and 11000H_1.csv\n",
      "Loaded pair: 11000L_10.csv and 11000H_10.csv\n",
      "Loaded pair: 11000L_11.csv and 11000H_11.csv\n",
      "Loaded pair: 11000L_12.csv and 11000H_12.csv\n",
      "Loaded pair: 11000L_13.csv and 11000H_13.csv\n",
      "Loaded pair: 11000L_14.csv and 11000H_14.csv\n",
      "Loaded pair: 11000L_15.csv and 11000H_15.csv\n",
      "Loaded pair: 11000L_16.csv and 11000H_16.csv\n",
      "Loaded pair: 11000L_17.csv and 11000H_17.csv\n",
      "Loaded pair: 11000L_18.csv and 11000H_18.csv\n",
      "Loaded pair: 11000L_19.csv and 11000H_19.csv\n",
      "Loaded pair: 11000L_2.csv and 11000H_2.csv\n",
      "Loaded pair: 11000L_20.csv and 11000H_20.csv\n",
      "Loaded pair: 11000L_3.csv and 11000H_3.csv\n",
      "Loaded pair: 11000L_4.csv and 11000H_4.csv\n",
      "Loaded pair: 11000L_5.csv and 11000H_5.csv\n",
      "Loaded pair: 11000L_6.csv and 11000H_6.csv\n",
      "Loaded pair: 11000L_7.csv and 11000H_7.csv\n",
      "Loaded pair: 11000L_8.csv and 11000H_8.csv\n",
      "Loaded pair: 11000L_9.csv and 11000H_9.csv\n",
      "Loading drone segments from Data\\DroneRF\\AR drone\n",
      "Loaded pair: 10100L_0.csv and 10100H_0.csv\n",
      "Loaded pair: 10100L_1.csv and 10100H_1.csv\n",
      "Loaded pair: 10100L_10.csv and 10100H_10.csv\n",
      "Loaded pair: 10100L_11.csv and 10100H_11.csv\n",
      "Loaded pair: 10100L_12.csv and 10100H_12.csv\n",
      "Loaded pair: 10100L_13.csv and 10100H_13.csv\n",
      "Loaded pair: 10100L_14.csv and 10100H_14.csv\n",
      "Loaded pair: 10100L_15.csv and 10100H_15.csv\n",
      "Loaded pair: 10100L_16.csv and 10100H_16.csv\n",
      "Loaded pair: 10100L_17.csv and 10100H_17.csv\n",
      "Loaded pair: 10100L_18.csv and 10100H_18.csv\n",
      "Loaded pair: 10100L_19.csv and 10100H_19.csv\n",
      "Loaded pair: 10100L_2.csv and 10100H_2.csv\n",
      "Loaded pair: 10100L_20.csv and 10100H_20.csv\n",
      "Loaded pair: 10100L_3.csv and 10100H_3.csv\n",
      "Loaded pair: 10100L_4.csv and 10100H_4.csv\n",
      "Loaded pair: 10100L_5.csv and 10100H_5.csv\n",
      "Loaded pair: 10100L_6.csv and 10100H_6.csv\n",
      "Loaded pair: 10100L_7.csv and 10100H_7.csv\n",
      "Loaded pair: 10100L_8.csv and 10100H_8.csv\n",
      "Loaded pair: 10100L_9.csv and 10100H_9.csv\n",
      "Loaded pair: 10101L_0.csv and 10101H_0.csv\n",
      "Loaded pair: 10101L_1.csv and 10101H_1.csv\n",
      "Loaded pair: 10101L_10.csv and 10101H_10.csv\n",
      "Loaded pair: 10101L_11.csv and 10101H_11.csv\n",
      "Loaded pair: 10101L_12.csv and 10101H_12.csv\n",
      "Loaded pair: 10101L_13.csv and 10101H_13.csv\n",
      "Loaded pair: 10101L_14.csv and 10101H_14.csv\n",
      "Loaded pair: 10101L_15.csv and 10101H_15.csv\n",
      "Loaded pair: 10101L_16.csv and 10101H_16.csv\n",
      "Loaded pair: 10101L_17.csv and 10101H_17.csv\n",
      "Loaded pair: 10101L_18.csv and 10101H_18.csv\n",
      "Loaded pair: 10101L_19.csv and 10101H_19.csv\n",
      "Loaded pair: 10101L_2.csv and 10101H_2.csv\n",
      "Loaded pair: 10101L_20.csv and 10101H_20.csv\n",
      "Loaded pair: 10101L_3.csv and 10101H_3.csv\n",
      "Loaded pair: 10101L_4.csv and 10101H_4.csv\n",
      "Loaded pair: 10101L_5.csv and 10101H_5.csv\n",
      "Loaded pair: 10101L_6.csv and 10101H_6.csv\n",
      "Loaded pair: 10101L_7.csv and 10101H_7.csv\n",
      "Loaded pair: 10101L_8.csv and 10101H_8.csv\n",
      "Loaded pair: 10101L_9.csv and 10101H_9.csv\n",
      "Loaded pair: 10110L_0.csv and 10110H_0.csv\n",
      "Loaded pair: 10110L_1.csv and 10110H_1.csv\n",
      "Loaded pair: 10110L_10.csv and 10110H_10.csv\n",
      "Loaded pair: 10110L_11.csv and 10110H_11.csv\n",
      "Loaded pair: 10110L_12.csv and 10110H_12.csv\n",
      "Loaded pair: 10110L_13.csv and 10110H_13.csv\n",
      "Loaded pair: 10110L_14.csv and 10110H_14.csv\n",
      "Loaded pair: 10110L_15.csv and 10110H_15.csv\n",
      "Loaded pair: 10110L_16.csv and 10110H_16.csv\n",
      "Loaded pair: 10110L_17.csv and 10110H_17.csv\n",
      "Loaded pair: 10110L_18.csv and 10110H_18.csv\n",
      "Loaded pair: 10110L_19.csv and 10110H_19.csv\n",
      "Loaded pair: 10110L_2.csv and 10110H_2.csv\n",
      "Loaded pair: 10110L_20.csv and 10110H_20.csv\n",
      "Loaded pair: 10110L_3.csv and 10110H_3.csv\n",
      "Loaded pair: 10110L_4.csv and 10110H_4.csv\n",
      "Loaded pair: 10110L_5.csv and 10110H_5.csv\n",
      "Loaded pair: 10110L_6.csv and 10110H_6.csv\n",
      "Loaded pair: 10110L_7.csv and 10110H_7.csv\n",
      "Loaded pair: 10110L_8.csv and 10110H_8.csv\n",
      "Loaded pair: 10110L_9.csv and 10110H_9.csv\n",
      "Loaded pair: 10111L_0.csv and 10111H_0.csv\n",
      "Loaded pair: 10111L_1.csv and 10111H_1.csv\n",
      "Loaded pair: 10111L_10.csv and 10111H_10.csv\n",
      "Loaded pair: 10111L_11.csv and 10111H_11.csv\n",
      "Loaded pair: 10111L_12.csv and 10111H_12.csv\n",
      "Loaded pair: 10111L_13.csv and 10111H_13.csv\n",
      "Loaded pair: 10111L_14.csv and 10111H_14.csv\n",
      "Loaded pair: 10111L_15.csv and 10111H_15.csv\n",
      "Loaded pair: 10111L_16.csv and 10111H_16.csv\n",
      "Loaded pair: 10111L_17.csv and 10111H_17.csv\n",
      "Loaded pair: 10111L_2.csv and 10111H_2.csv\n",
      "Loaded pair: 10111L_3.csv and 10111H_3.csv\n",
      "Loaded pair: 10111L_4.csv and 10111H_4.csv\n",
      "Loaded pair: 10111L_5.csv and 10111H_5.csv\n",
      "Loaded pair: 10111L_6.csv and 10111H_6.csv\n",
      "Loaded pair: 10111L_7.csv and 10111H_7.csv\n",
      "Loaded pair: 10111L_8.csv and 10111H_8.csv\n",
      "Loaded pair: 10111L_9.csv and 10111H_9.csv\n",
      "Loading background segments from Data\\DroneRF\\Background RF activites\n",
      "Loaded pair: 00000L_0.csv and 00000H_0.csv\n",
      "Loaded pair: 00000L_1.csv and 00000H_1.csv\n",
      "Loaded pair: 00000L_10.csv and 00000H_10.csv\n",
      "Loaded pair: 00000L_11.csv and 00000H_11.csv\n",
      "Loaded pair: 00000L_12.csv and 00000H_12.csv\n",
      "Loaded pair: 00000L_13.csv and 00000H_13.csv\n",
      "Loaded pair: 00000L_14.csv and 00000H_14.csv\n",
      "Loaded pair: 00000L_15.csv and 00000H_15.csv\n",
      "Loaded pair: 00000L_16.csv and 00000H_16.csv\n",
      "Loaded pair: 00000L_17.csv and 00000H_17.csv\n",
      "Loaded pair: 00000L_18.csv and 00000H_18.csv\n",
      "Loaded pair: 00000L_19.csv and 00000H_19.csv\n",
      "Loaded pair: 00000L_2.csv and 00000H_2.csv\n",
      "Loaded pair: 00000L_20.csv and 00000H_20.csv\n",
      "Loaded pair: 00000L_21.csv and 00000H_21.csv\n",
      "Loaded pair: 00000L_22.csv and 00000H_22.csv\n",
      "Loaded pair: 00000L_23.csv and 00000H_23.csv\n",
      "Loaded pair: 00000L_24.csv and 00000H_24.csv\n",
      "Loaded pair: 00000L_25.csv and 00000H_25.csv\n",
      "Loaded pair: 00000L_26.csv and 00000H_26.csv\n",
      "Loaded pair: 00000L_27.csv and 00000H_27.csv\n",
      "Loaded pair: 00000L_28.csv and 00000H_28.csv\n",
      "Loaded pair: 00000L_29.csv and 00000H_29.csv\n",
      "Loaded pair: 00000L_3.csv and 00000H_3.csv\n",
      "Loaded pair: 00000L_30.csv and 00000H_30.csv\n",
      "Loaded pair: 00000L_31.csv and 00000H_31.csv\n",
      "Loaded pair: 00000L_32.csv and 00000H_32.csv\n",
      "Loaded pair: 00000L_33.csv and 00000H_33.csv\n",
      "Loaded pair: 00000L_34.csv and 00000H_34.csv\n",
      "Loaded pair: 00000L_35.csv and 00000H_35.csv\n",
      "Loaded pair: 00000L_36.csv and 00000H_36.csv\n",
      "Loaded pair: 00000L_37.csv and 00000H_37.csv\n",
      "Loaded pair: 00000L_38.csv and 00000H_38.csv\n",
      "Loaded pair: 00000L_39.csv and 00000H_39.csv\n",
      "Loaded pair: 00000L_4.csv and 00000H_4.csv\n",
      "Loaded pair: 00000L_40.csv and 00000H_40.csv\n",
      "Loaded pair: 00000L_5.csv and 00000H_5.csv\n",
      "Loaded pair: 00000L_6.csv and 00000H_6.csv\n",
      "Loaded pair: 00000L_7.csv and 00000H_7.csv\n",
      "Loaded pair: 00000L_8.csv and 00000H_8.csv\n",
      "Loaded pair: 00000L_9.csv and 00000H_9.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't_seg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     y.append(\u001b[32m0\u001b[39m)\n\u001b[32m     16\u001b[39m non_drone_csv_path = \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mData\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpluto_sdr_iq_data_20250320_122718.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m X, y = add_csv_data_to_training(non_drone_csv_path, X, y, class_label=\u001b[32m0\u001b[39m, fs=fs, t_seg=\u001b[43mt_seg\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 't_seg' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for folder in drone_folders:\n",
    "    print(f\"Loading drone segments from {folder}\")\n",
    "    segment_pairs = load_segment_pair_files(folder)\n",
    "    for (sig_low, sig_high, file_low, file_high) in segment_pairs:\n",
    "        features = extract_all_features(sig_low, sig_high, fs)\n",
    "        X.append(features)\n",
    "        y.append(1)\n",
    "\n",
    "\n",
    "print(f\"Loading background segments from {background_folder}\")\n",
    "background_segment_pairs = load_segment_pair_files(background_folder)\n",
    "for (sig_low, sig_high, file_low, file_high) in background_segment_pairs:\n",
    "    features = extract_all_features(sig_low, sig_high, fs)\n",
    "    X.append(features)\n",
    "    y.append(0)\n",
    "non_drone_csv_path = r'Data\\pluto_sdr_iq_data_20250320_122718.csv'\n",
    "X, y = add_csv_data_to_training(non_drone_csv_path, X, y, class_label=0, fs=fs, t_seg=t_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: Data\\pluto_sdr_iq_data_20250320_122718.csv\n",
      "Found 37 complete segments in CSV file\n",
      "Added 37 segments with class label 0 to training data\n"
     ]
    }
   ],
   "source": [
    "X, y = add_csv_data_to_training(non_drone_csv_path, X, y, class_label=0, fs=fs, t_seg=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (264, 1024)\n",
      "Labels shape: (264,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60006784, 0.30210175, 0.52592163, ..., 0.25788792, 0.06525057,\n",
       "        0.28750348],\n",
       "       [0.25488565, 0.29858111, 0.25247625, ..., 0.13910876, 0.32933118,\n",
       "        0.49817144],\n",
       "       [0.39971281, 0.29168567, 0.14245768, ..., 0.44475691, 0.05480315,\n",
       "        0.41836384],\n",
       "       ...,\n",
       "       [1.        , 0.0985408 , 0.03899714, ..., 0.03353172, 0.01515668,\n",
       "        0.03891594],\n",
       "       [1.        , 0.02726214, 0.00406939, ..., 0.01674093, 0.03206413,\n",
       "        0.02320978],\n",
       "       [1.        , 0.07862266, 0.0201619 , ..., 0.01455576, 0.01978327,\n",
       "        0.01836937]], shape=(264, 1024))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled training data saved to 'train_resampled.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = X_train.columns if hasattr(X_train, 'columns') else [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "\n",
    "df_train_resampled = pd.DataFrame(X_train_resampled, columns=columns)\n",
    "df_train_resampled['label'] = y_train_resampled \n",
    "\n",
    "# Save to CSV\n",
    "df_train_resampled.to_csv('train_resampled.csv', index=False)\n",
    "print(\"Resampled training data saved to 'train_resampled.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_1015</th>\n",
       "      <th>feature_1016</th>\n",
       "      <th>feature_1017</th>\n",
       "      <th>feature_1018</th>\n",
       "      <th>feature_1019</th>\n",
       "      <th>feature_1020</th>\n",
       "      <th>feature_1021</th>\n",
       "      <th>feature_1022</th>\n",
       "      <th>feature_1023</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.766627</td>\n",
       "      <td>0.578235</td>\n",
       "      <td>0.157086</td>\n",
       "      <td>0.568760</td>\n",
       "      <td>0.768131</td>\n",
       "      <td>0.428041</td>\n",
       "      <td>0.041847</td>\n",
       "      <td>0.045593</td>\n",
       "      <td>0.209855</td>\n",
       "      <td>0.029573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156485</td>\n",
       "      <td>0.119663</td>\n",
       "      <td>0.522185</td>\n",
       "      <td>0.143958</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.394562</td>\n",
       "      <td>0.176607</td>\n",
       "      <td>0.352638</td>\n",
       "      <td>0.292205</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129720</td>\n",
       "      <td>0.398150</td>\n",
       "      <td>0.253483</td>\n",
       "      <td>0.282366</td>\n",
       "      <td>0.308898</td>\n",
       "      <td>0.064255</td>\n",
       "      <td>0.327565</td>\n",
       "      <td>0.282170</td>\n",
       "      <td>0.331814</td>\n",
       "      <td>0.356929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.367078</td>\n",
       "      <td>0.091329</td>\n",
       "      <td>0.170362</td>\n",
       "      <td>0.130283</td>\n",
       "      <td>0.233197</td>\n",
       "      <td>0.160886</td>\n",
       "      <td>0.179582</td>\n",
       "      <td>0.056798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627913</td>\n",
       "      <td>0.493194</td>\n",
       "      <td>0.091230</td>\n",
       "      <td>0.131886</td>\n",
       "      <td>0.443734</td>\n",
       "      <td>0.135997</td>\n",
       "      <td>0.603839</td>\n",
       "      <td>0.154673</td>\n",
       "      <td>0.185795</td>\n",
       "      <td>0.102805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416081</td>\n",
       "      <td>0.165423</td>\n",
       "      <td>0.148593</td>\n",
       "      <td>0.160592</td>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.144349</td>\n",
       "      <td>0.158086</td>\n",
       "      <td>0.426031</td>\n",
       "      <td>0.217403</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038643</td>\n",
       "      <td>0.009771</td>\n",
       "      <td>0.030809</td>\n",
       "      <td>0.022795</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.042167</td>\n",
       "      <td>0.027967</td>\n",
       "      <td>0.021877</td>\n",
       "      <td>0.038506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018463</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.024889</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>0.015382</td>\n",
       "      <td>0.030027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.284546</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.149473</td>\n",
       "      <td>0.269185</td>\n",
       "      <td>0.225193</td>\n",
       "      <td>0.066250</td>\n",
       "      <td>0.405332</td>\n",
       "      <td>0.226789</td>\n",
       "      <td>0.390357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458612</td>\n",
       "      <td>0.193191</td>\n",
       "      <td>0.435854</td>\n",
       "      <td>0.158643</td>\n",
       "      <td>0.274269</td>\n",
       "      <td>0.141692</td>\n",
       "      <td>0.291278</td>\n",
       "      <td>0.131933</td>\n",
       "      <td>0.214344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0   0.766627   0.578235   0.157086   0.568760   0.768131   0.428041   \n",
       "1   0.129720   0.398150   0.253483   0.282366   0.308898   0.064255   \n",
       "2   0.627913   0.493194   0.091230   0.131886   0.443734   0.135997   \n",
       "3   1.000000   0.038643   0.009771   0.030809   0.022795   0.033609   \n",
       "4   0.284546   0.454379   0.146484   0.149473   0.269185   0.225193   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_1015  \\\n",
       "0   0.041847   0.045593   0.209855   0.029573  ...      0.156485   \n",
       "1   0.327565   0.282170   0.331814   0.356929  ...      0.034543   \n",
       "2   0.603839   0.154673   0.185795   0.102805  ...      0.416081   \n",
       "3   0.042167   0.027967   0.021877   0.038506  ...      0.018463   \n",
       "4   0.066250   0.405332   0.226789   0.390357  ...      0.458612   \n",
       "\n",
       "   feature_1016  feature_1017  feature_1018  feature_1019  feature_1020  \\\n",
       "0      0.119663      0.522185      0.143958      0.172600      0.394562   \n",
       "1      0.367078      0.091329      0.170362      0.130283      0.233197   \n",
       "2      0.165423      0.148593      0.160592      0.084939      0.144349   \n",
       "3      0.022437      0.016195      0.005150      0.011961      0.024889   \n",
       "4      0.193191      0.435854      0.158643      0.274269      0.141692   \n",
       "\n",
       "   feature_1021  feature_1022  feature_1023  label  \n",
       "0      0.176607      0.352638      0.292205      1  \n",
       "1      0.160886      0.179582      0.056798      1  \n",
       "2      0.158086      0.426031      0.217403      1  \n",
       "3      0.004297      0.015382      0.030027      0  \n",
       "4      0.291278      0.131933      0.214344      1  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data saved to 'test.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = pd.DataFrame(X_test, columns=columns)\n",
    "df_test['label'] = y_test\n",
    "\n",
    "# Save to CSV\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "print(\"Test data saved to 'test.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation...\n",
      "Cross-validation accuracy: 0.9429 (±0.0541)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Performing 10-fold cross-validation...\")\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "cv_scores = cross_val_score(xgb_model, X_train_resampled, y_train_resampled, cv=kfold, scoring='accuracy')\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the trained model using pickle or XGBoost's own save_model method\n",
    "with open('xgb_model_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.44      0.58        16\n",
      "           1       0.80      0.97      0.88        37\n",
      "\n",
      "    accuracy                           0.81        53\n",
      "   macro avg       0.84      0.71      0.73        53\n",
      "weighted avg       0.82      0.81      0.79        53\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 7  9]\n",
      " [ 1 36]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation...\n",
      "Cross-validation accuracy: 0.8890 (±0.0667)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define SVM parameters (adjust based on your problem)\n",
    "svm_params = {\n",
    "    'kernel': 'linear',       # Common kernels: 'linear', 'rbf', 'poly'\n",
    "    'C': 1.0,              # Regularization parameter (higher = stricter margin)\n",
    "    'gamma': 'scale',      # Kernel coefficient ('scale' or 'auto')\n",
    "    'random_state': 42,\n",
    "    'probability': True    # Enable probability estimates (for ROC-AUC)\n",
    "}\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC(**svm_params)\n",
    "\n",
    "# Cross-validation using 10-fold\n",
    "print(\"Performing 10-fold cross-validation...\")\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    svm_model, \n",
    "    X_train_resampled, \n",
    "    y_train_resampled, \n",
    "    cv=kfold, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Save the trained model using pickle or XGBoost's own save_model method\n",
    "with open('svm_model_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.25      0.17         4\n",
      "           1       0.92      0.83      0.88        42\n",
      "\n",
      "    accuracy                           0.78        46\n",
      "   macro avg       0.52      0.54      0.52        46\n",
      "weighted avg       0.85      0.78      0.81        46\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1  3]\n",
      " [ 7 35]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(X_test)\n",
    "y_pred_proba = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=10, random_state=42, shuffle=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for optimal parameters...\n",
      "Fitting 10 folds for each of 729 candidates, totalling 7290 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     11\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     12\u001b[39m     estimator=xgb.XGBClassifier(objective=\u001b[33m'\u001b[39m\u001b[33mbinary:logistic\u001b[39m\u001b[33m'\u001b[39m, eval_metric=\u001b[33m'\u001b[39m\u001b[33mlogloss\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m),\n\u001b[32m     13\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting grid search for optimal parameters...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest parameters found:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(grid_search.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GD-drone\\gddrone\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'eta': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search for optimal parameters...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        46\n",
      "   macro avg       1.00      1.00      1.00        46\n",
      "weighted avg       1.00      1.00      1.00        46\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 8  0]\n",
      " [ 0 38]]\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate on test set\n",
    "# y_pred = best_model.predict(X_test_scaled)\n",
    "# y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Most Important Features:\n",
      "Feature 78: 0.0282\n",
      "Feature 317: 0.0248\n",
      "Feature 651: 0.0236\n",
      "Feature 66: 0.0187\n",
      "Feature 537: 0.0183\n",
      "Feature 816: 0.0182\n",
      "Feature 510: 0.0168\n",
      "Feature 0: 0.0155\n",
      "Feature 177: 0.0151\n",
      "Feature 708: 0.0146\n",
      "Feature 640: 0.0145\n",
      "Feature 992: 0.0140\n",
      "Feature 366: 0.0134\n",
      "Feature 471: 0.0134\n",
      "Feature 472: 0.0127\n",
      "Feature 690: 0.0120\n",
      "Feature 801: 0.0116\n",
      "Feature 638: 0.0109\n",
      "Feature 566: 0.0108\n",
      "Feature 171: 0.0107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "for i in range(min(20, len(sorted_idx))):\n",
    "    print(f\"Feature {sorted_idx[i]}: {feature_importance[sorted_idx[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('drone_detector_xgboost_smote_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on SDR csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: Data\\pluto_sdr_iq_data_20250320_122718.csv\n",
      "Total segments found: 37\n",
      "Model loaded from: xgb_model_scaled.pkl\n",
      "Segment 1: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 2: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 3: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 4: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 5: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 6: Prediction = 0 (Confidence: 0.0050)\n",
      "Segment 7: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 8: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 9: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 10: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 11: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 12: Prediction = 0 (Confidence: 0.0061)\n",
      "Segment 13: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 14: Prediction = 0 (Confidence: 0.0078)\n",
      "Segment 15: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 16: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 17: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 18: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 19: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 20: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 21: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 22: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 23: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 24: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 25: Prediction = 0 (Confidence: 0.0055)\n",
      "Segment 26: Prediction = 0 (Confidence: 0.0056)\n",
      "Segment 27: Prediction = 0 (Confidence: 0.0061)\n",
      "Segment 28: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 29: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 30: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 31: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 32: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 33: Prediction = 0 (Confidence: 0.0061)\n",
      "Segment 34: Prediction = 0 (Confidence: 0.0051)\n",
      "Segment 35: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 36: Prediction = 0 (Confidence: 0.0046)\n",
      "Segment 37: Prediction = 0 (Confidence: 0.0056)\n",
      "\n",
      "Final Predictions for each segment:\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "Drone detected in 0 out of 37 segments.\n",
      "\n",
      "Final decision based on majority voting: NO DRONE DETECTED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import signal\n",
    "\n",
    "# def moving_average_filter(data, window_size=5):\n",
    "#     \"\"\"\n",
    "#     Applies a moving average filter (FIR filter with uniform weights)\n",
    "#     to smooth the input data.\n",
    "#     \"\"\"\n",
    "#     return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def extract_dft_features(signal_data, n_fft=2048):\n",
    "    \"\"\"\n",
    "    Extract DFT magnitude spectrum features as described in the research paper.\n",
    "    \n",
    "    Parameters:\n",
    "        signal_data: Input signal data\n",
    "        n_fft: Number of points for FFT (default: 2048 as used in the paper)\n",
    "    \n",
    "    Returns:\n",
    "        Magnitude spectrum features\n",
    "    \"\"\"\n",
    "    # Compute DFT\n",
    "    dft = np.fft.fft(signal_data, n=n_fft)\n",
    "    \n",
    "    # Extract one-sided magnitude spectrum (first half of the spectrum)\n",
    "    magnitude_spectrum = np.abs(dft[:n_fft//2])\n",
    "    \n",
    "    # Normalize the magnitude spectrum\n",
    "    if np.max(magnitude_spectrum) > 0:\n",
    "        magnitude_spectrum = magnitude_spectrum / np.max(magnitude_spectrum)\n",
    "    \n",
    "    return magnitude_spectrum\n",
    "\n",
    "\n",
    "\n",
    "def extract_all_features(signal_low, signal_high, fs):\n",
    "    \"\"\"\n",
    "    Extract features strictly following the research paper:\n",
    "    use only the lower band DFT magnitude spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "        signal_low: Low band signal data\n",
    "        signal_high: High band signal data (ignored)\n",
    "        fs: Sampling rate (not used in this extraction)\n",
    "    \n",
    "    Returns:\n",
    "        Feature vector of length 1024 (one-sided DFT magnitude spectrum)\n",
    "    \"\"\"\n",
    "    # filtered_low = moving_average_filter(signal_low)\n",
    "    dft_features_low = extract_dft_features(signal_low)\n",
    "    return dft_features_low\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load a trained classification model from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model loaded from: {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def visualize_results(predictions, segment_numbers):\n",
    "    \"\"\"Visualize prediction results for each segment.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(segment_numbers, predictions, color=['green' if p == 1 else 'red' for p in predictions])\n",
    "    plt.axhline(y=0.5, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Segment Number')\n",
    "    plt.ylabel('Prediction (1=Drone, 0=Background)')\n",
    "    plt.title('Drone Detection Results by Segment')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.xticks(segment_numbers)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    for i, v in enumerate(predictions):\n",
    "        plt.text(segment_numbers[i], v+0.05, str(v), ha='center')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_features(feature_vector, n_dft=1024, n_stats=12, n_psd=10):\n",
    "    \"\"\"Visualize different parts of the feature vector.\"\"\"\n",
    "    # Separate different feature types\n",
    "    dft_features = feature_vector[:n_dft]\n",
    "    stats_features = feature_vector[n_dft:n_dft+n_stats]\n",
    "    psd_features = feature_vector[n_dft+n_stats:]\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot DFT features\n",
    "    axs[0].plot(dft_features)\n",
    "    axs[0].set_title('DFT Magnitude Spectrum Features')\n",
    "    axs[0].set_xlabel('Frequency Bin')\n",
    "    axs[0].set_ylabel('Normalized Magnitude')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot statistical features\n",
    "    stats_labels = ['Mean', 'Std', 'Max', 'Min', 'Median', '25th', '75th', 'Var', \n",
    "                   'RMS', 'MAD', 'Above Mean', 'Rising Edges']\n",
    "    axs[1].bar(stats_labels, stats_features)\n",
    "    axs[1].set_title('Statistical Features')\n",
    "    axs[1].set_ylabel('Value')\n",
    "    plt.setp(axs[1].get_xticklabels(), rotation=45, ha='right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    # Plot PSD statistical features\n",
    "    psd_labels = ['Mean', 'Std', 'Max', 'Min', 'Median', '25th', '75th', 'Var', \n",
    "                 'Spectral Centroid', 'Spectral Bandwidth']\n",
    "    axs[2].bar(psd_labels, psd_features)\n",
    "    axs[2].set_title('PSD Statistical Features')\n",
    "    axs[2].set_ylabel('Value')\n",
    "    plt.setp(axs[2].get_xticklabels(), rotation=45, ha='right')\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Parameters\n",
    "    fs = 40e6  # Sampling rate: 40 MHz\n",
    "    window_size = 5  # For moving average filter\n",
    "    t_seg = 20  # Time segment in ms\n",
    "    n_fft = 2048  # FFT size for DFT features\n",
    "    nperseg = 1024  # For Welch's method\n",
    "    \n",
    "   \n",
    "    csv_path = r'Data\\pluto_sdr_iq_data_20250320_122718.csv'\n",
    "    \n",
    "   \n",
    "    print(f\"Loading data from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "\n",
    "    magnitude_data = df['Magnitude'].values\n",
    "    \n",
    "\n",
    "    total_samples = len(magnitude_data)\n",
    "    samples_per_band = int(t_seg / 1000 * fs)\n",
    "    segment_length = 2 * samples_per_band\n",
    "    \n",
    "    if total_samples < segment_length:\n",
    "        print(\"Error: Not enough samples in the CSV file to form a single segment.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    num_segments = total_samples // segment_length\n",
    "    print(f\"Total segments found: {num_segments}\")\n",
    "    \n",
    "\n",
    "    model_path = 'xgb_model_scaled.pkl'\n",
    "    model = load_model(model_path)\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    features_list = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        segment = magnitude_data[i * segment_length : (i + 1) * segment_length]\n",
    "        \n",
    " \n",
    "        low_band = segment[:samples_per_band]\n",
    "        high_band = segment[samples_per_band:]\n",
    "        \n",
    "   \n",
    "        feature_vector = extract_all_features(low_band, high_band, fs)\n",
    "        features_list.append(feature_vector)\n",
    "        \n",
    "  \n",
    "        feature_vector_reshaped = feature_vector.reshape(1, -1)\n",
    "        pred = model.predict(feature_vector_reshaped)\n",
    "        predictions.append(pred[0])\n",
    "        \n",
    "        pred_prob = model.predict_proba(feature_vector_reshaped)\n",
    "        print(f\"Segment {i+1}: Prediction = {pred[0]} (Confidence: {pred_prob[0][1]:.4f})\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nFinal Predictions for each segment:\")\n",
    "    print(predictions)\n",
    "    print(f\"Drone detected in {sum(predictions)} out of {len(predictions)} segments.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    if num_segments > 0:\n",
    "        \n",
    "        final_decision = 1 if sum(predictions)/len(predictions) > 0.5 else 0\n",
    "        print(f\"\\nFinal decision based on majority voting: {'DRONE DETECTED' if final_decision == 1 else 'NO DRONE DETECTED'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
